{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доп задание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Изменим функцию из 1 лабораторной работы\n",
    "def conv2D_my_help(input, weight, stride, padding,output_padding):\n",
    "    batch_size, in_channels, img_height, img_width = input.shape\n",
    "    out_channels, in_channels, kernel_height, kernel_width = weight.shape\n",
    "    h_out = (img_height - 1) * stride - 2 * padding + kernel_height + output_padding\n",
    "    w_out = (img_width - 1) * stride - 2 * padding + kernel_width + output_padding\n",
    "\n",
    "    result = np.zeros((batch_size, out_channels,h_out, w_out))\n",
    "\n",
    "    return (batch_size, out_channels, h_out, w_out , \n",
    "            in_channels, kernel_height, \n",
    "            kernel_width, img_height, img_width, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем транспонированноую свёртку\n",
    "def conv2D_T_my(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):\n",
    "    \n",
    "    (batch_size, out_channels, H_out, \n",
    "    W_out, in_channels,\n",
    "    kernel_height, kernel_width,\n",
    "    in_height, in_width, result) = conv2D_my_help(input, weight,\n",
    "                                                       stride, padding, output_padding=output_padding)\n",
    "    \n",
    "    if padding > 0:\n",
    "        input = np.pad(input, padding, mode='constant')\n",
    "\n",
    "    result = np.zeros((batch_size, out_channels, H_out, W_out))\n",
    "\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        for channel_o in range(out_channels):\n",
    "            for h_o in range(H_out):\n",
    "                for w_o in range(W_out):\n",
    "                    for in_c in range(in_channels):\n",
    "                        for kern_h in range(kernel_height):\n",
    "                            for kern_w in range(kernel_width):\n",
    "                                ii = h_o + padding - kern_h * dilation\n",
    "                                jj = w_o + padding - kern_w * dilation\n",
    "                                if ii >= 0 and jj >= 0 and ii < in_height * stride and jj < in_width * stride and (ii % stride == 0) and (jj % stride == 0):\n",
    "                                    ii //= stride\n",
    "                                    jj //= stride\n",
    "                                    result[batch, channel_o, h_o, w_o] += np.multiply(input[batch, in_c, ii, jj], weight[channel_o, in_c, kern_h, kern_w])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основное задание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0\\\n",
    "                          , output_padding=0, bias=True, dilation=1, padding_mode='zeros'):\n",
    "    #Обёртка\n",
    "    def wrapper(mat):\n",
    "        \n",
    "        #Проверки параметров\n",
    "        if output_padding < 0:\n",
    "            raise Exception(f\"Padiing should be 0 or positive\")\n",
    "        if stride < 0:\n",
    "            raise Exception(f\"Stride should be 0 or positive\")\n",
    "        if (padding_mode != 'zeros'):\n",
    "            raise Exception(f\"Ivalid padding_mode\")\n",
    "            \n",
    "        #Смещение\n",
    "        if bias:\n",
    "            bias_value = torch.rand(out_channels)\n",
    "        else:\n",
    "            bias_value = torch.zeros(out_channels)\n",
    "            \n",
    "        \n",
    "        #Фильтр с учётом размера ядра\n",
    "        if type(kernel_size) == tuple:\n",
    "            flter = torch.rand(in_channels, out_channels, kernel_size[0], kernel_size[1])\n",
    "        elif type(kernel_size) == int:\n",
    "            flter = torch.rand(in_channels, out_channels, kernel_size, kernel_size)\n",
    "        else:\n",
    "            raise Exception(f\"Ivalid kernel_size type\")\n",
    "            \n",
    "            \n",
    "        #\"Обход\" фильтром\n",
    "        res = []\n",
    "        for ochnl in range(out_channels):\n",
    "            feature_map = torch.zeros((mat.shape[1] - 1) * stride + dilation * (flter.shape[2] - 1) + 1\\\n",
    "                                            , (mat.shape[2] - 1) * stride + dilation * (flter.shape[3] - 1) + 1)\n",
    "            \n",
    "            for ichnl in range(in_channels):\n",
    "                for i in range(0, mat.shape[1]):\n",
    "                    for j in range(0, mat.shape[2]):\n",
    "                        cur = mat[ichnl][i][j]\n",
    "                        val = cur * flter[ichnl][ochnl]\n",
    "                        zeros = torch.zeros((flter.shape[2] - 1) * dilation + 1, (flter.shape[3] - 1) * dilation + 1)\n",
    "                        for k in range(0, zeros.shape[0], dilation):\n",
    "                            for f in range(0, zeros.shape[1], dilation):\n",
    "                                zeros[k][f] = val[k // dilation][f // dilation]\n",
    "                        total = np.add((zeros), feature_map[i * stride:i * stride + dilation * (flter.shape[2] - 1) + 1\\\n",
    "                                                            , j * stride:j * stride + dilation * (flter.shape[3] - 1) + 1])\n",
    "                        \n",
    "                        feature_map[i * stride:i * stride + dilation * (flter.shape[2] - 1) + 1\\\n",
    "                                    , j * stride:j * stride + dilation * (flter.shape[3] - 1) + 1] = total\n",
    "\n",
    "            res.append(np.add(feature_map, np.full((feature_map.shape), bias_value[ochnl])))\n",
    "        \n",
    "        for l in range(len(res)):\n",
    "            if output_padding > 0:\n",
    "                pad = torch.nn.ConstantPad1d((0, output_padding, 0, output_padding), 0)\n",
    "                res[l] = pad(res[l])\n",
    "            res[l] = res[l][padding:res[l].shape[0] - padding, padding:res[l].shape[1] - padding]\n",
    "            \n",
    "        return np.array(res), np.array(flter), np.array(bias_value)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тесты основного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = torch.rand(3, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "def test1():\n",
    "    customConv = customConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=10, padding=0\\\n",
    "                                      , output_padding=0, bias=True, dilation=3, padding_mode='zeros')\n",
    "    \n",
    "    result, flter, bias_value = customConv(test_data_1)\n",
    "    torchConv = torch.nn.ConvTranspose2d(in_channels=3, out_channels=2, kernel_size=3, stride=10, padding=0\\\n",
    "                                      , output_padding=0, bias=True, dilation=3, padding_mode='zeros')\n",
    "    \n",
    "    torchConv.weight.data = torch.tensor(flter)\n",
    "    torchConv.bias.data = torch.tensor(bias_value)\n",
    "    customResult = str(np.round(result,2))\n",
    "    torchResult = str(np.round(np.array(torchConv(test_data_1).data),2))\n",
    "    assert customResult == torchResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тесты доп задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0701,  0.2319, -1.1459,  1.6100, -0.5627,  4.1986, -2.3870],\n",
      "          [-0.1406,  0.5679,  0.0813,  1.4064, -0.0839,  8.1608, -2.8441],\n",
      "          [-0.3351, -0.0324,  0.1492,  5.7634, -1.4600,  2.5127, -1.8647],\n",
      "          [ 0.2987,  1.5988,  1.2388,  7.9291,  3.1784, -1.8894,  1.4940],\n",
      "          [-0.2612,  0.3133, -0.8188, -4.7213, -0.1466, -5.3322,  2.0261],\n",
      "          [ 0.2582,  2.7574,  1.4441, -1.7009,  0.6177, -0.4861,  0.7607],\n",
      "          [-1.5094, -4.4170, -4.4054, -3.3296, -2.0211,  0.5816,  0.1466]]]])\n",
      "tensor([[[[-0.0701,  0.2319, -1.1459,  1.6100, -0.5627,  4.1986, -2.3870],\n",
      "          [-0.1406,  0.5679,  0.0813,  1.4064, -0.0839,  8.1608, -2.8441],\n",
      "          [-0.3351, -0.0324,  0.1492,  5.7634, -1.4600,  2.5127, -1.8647],\n",
      "          [ 0.2987,  1.5988,  1.2388,  7.9291,  3.1784, -1.8894,  1.4940],\n",
      "          [-0.2612,  0.3133, -0.8188, -4.7213, -0.1466, -5.3322,  2.0261],\n",
      "          [ 0.2582,  2.7574,  1.4441, -1.7009,  0.6177, -0.4861,  0.7607],\n",
      "          [-1.5094, -4.4170, -4.4054, -3.3296, -2.0211,  0.5816,  0.1466]]]])\n"
     ]
    }
   ],
   "source": [
    "image = torch.randn(1, 1, 5, 5) \n",
    "weight = torch.randn(1, 1, 3, 3)\n",
    "    \n",
    "myConv2D_T = torch.from_numpy(conv2D_T_my(image.numpy(), weight.numpy()))\n",
    "\n",
    "\n",
    "torchConv2D_T = F.conv_transpose2d(image, weight)\n",
    "\n",
    "myConv2D_T = myConv2D_T.to(torchConv2D_T.dtype)\n",
    "\n",
    "print(myConv2D_T)\n",
    "print(torchConv2D_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1():\n",
    "    image = torch.randn(1, 1, 5, 5) \n",
    "    weight = torch.randn(1, 1, 3, 3)\n",
    "    \n",
    "    myConv2D_T = torch.from_numpy(conv2D_T_my(image.numpy(), weight.numpy()))\n",
    "\n",
    "\n",
    "    torchConv2D_T = F.conv_transpose2d(image, weight)\n",
    "\n",
    "    myConv2D_T = myConv2D_T.to(torchConv2D_T.dtype)\n",
    "\n",
    "    assert torch.allclose(myConv2D_T, torchConv2D_T)\n",
    "\n",
    "def test_2():\n",
    "    image = torch.randn(1, 1, 7, 7) \n",
    "    weight = torch.randn(1, 1, 2, 2) \n",
    "    \n",
    "    myConv2D_T = torch.from_numpy(conv2D_T_my(image.numpy(), weight.numpy()))\n",
    "\n",
    "\n",
    "    torchConv2D_T = F.conv_transpose2d(image, weight)\n",
    "\n",
    "    myConv2D_T = myConv2D_T.to(torchConv2D_T.dtype)\n",
    "\n",
    "    assert torch.allclose(myConv2D_T, torchConv2D_T)\n",
    "\n",
    "def test_3():\n",
    "    image = torch.randn(1, 1, 4, 4) \n",
    "    weight = torch.randn(1, 1, 4, 4) \n",
    "    \n",
    "    myConv2D_T = torch.from_numpy(conv2D_T_my(image.numpy(), weight.numpy()))\n",
    "\n",
    "    torchConv2D_T = F.conv_transpose2d(image, weight)\n",
    "\n",
    "    myConv2D_T = myConv2D_T.to(torchConv2D_T.dtype)\n",
    "\n",
    "    assert torch.allclose(myConv2D_T, torchConv2D_T)\n",
    "\n",
    "def test_4():\n",
    "    image = torch.randn(1, 1, 6, 6) \n",
    "    weight = torch.randn(1, 1, 2, 2) \n",
    "    \n",
    "    myConv2D_T = torch.from_numpy(conv2D_T_my(image.numpy(), weight.numpy()))\n",
    "\n",
    "    torchConv2D_T =F.conv_transpose2d(image, weight)\n",
    "\n",
    "    myConv2D_T = myConv2D_T.to(torchConv2D_T.dtype)\n",
    "\n",
    "    assert torch.allclose(myConv2D_T, torchConv2D_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Успешный тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_1()\n",
    "    test_2()\n",
    "    test_3()\n",
    "    test_4()\n",
    "    print('ok')\n",
    "except BaseException as e:\n",
    "    print('error')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Неуспешный тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "The size of tensor a (7) must match the size of tensor b (12) at non-singleton dimension 3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_1()\n",
    "    test_2()\n",
    "    test_3()\n",
    "    test_4()\n",
    "    print('ok')\n",
    "except BaseException as e:\n",
    "    print('error')\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
